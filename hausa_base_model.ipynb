{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpyWPK+jAwJQU33071H+ef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smaliyu/AfriNLP/blob/main/hausa_base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training  a Base Model for Hate Detection in Tweets"
      ],
      "metadata": {
        "id": "dI0keo9h2Nsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I attempt to train a model that detects hate speech in tweets."
      ],
      "metadata": {
        "id": "hEHHcUQd2X17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive to persist output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1whhNvgyoKD-",
        "outputId": "2cc88f33-ccb4-4b28-f086-6a332d70d11d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working directory to drive\n",
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLwa3NFD0dkH",
        "outputId": "86dbec69-6565-4cc2-f23e-1affe3c245bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo for access to the datasets and content\n",
        "!git clone https://github.com/smaliyu/AfriNLP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9086xkUpGPF",
        "outputId": "686eab3a-aba8-4ddd-c5c1-8c5f32bdc278"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AfriNLP'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (8/8), 645.27 KiB | 2.14 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working directory to the root directory of repo\n",
        "%cd AfriNLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyvaAylhpTdq",
        "outputId": "a8fa026d-61de-48e2-9eb3-69ed71fb4688"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AfriNLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVNFzMl20pHr",
        "outputId": "5e89bb93-8706-4a83-b307-2ff379c1bd4b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AfriNLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "km0oki7JJuV4"
      },
      "outputs": [],
      "source": [
        "# necessary imports\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "\n",
        "# Custom dataset class to prepare dataset for the encoder model\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/AfriNLP/datasets/hausa(1).csv')\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = data['tweet'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "kmJ6Nr4up60k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the mapping of encoded labels to original labels\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgJ7-yIur6o4",
        "outputId": "db76630b-0d46-4459-c8a0-105d83d2d679"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Hate': 0, 'Indeterminate': 1, 'Normal': 2, 'Offensive': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJowXXcHvkOn",
        "outputId": "42bd67b0-65f1-4494-9e60-f4edb345dc43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "max_length = 128\n",
        "# Split the data into train and test sets first\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, encoded_labels, random_state=42, test_size=0.2)\n",
        "\n",
        "# Now tokenize each set\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, add_special_tokens=True, max_length=max_length, padding='max_length', return_attention_mask=True, truncation=True)\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "test_encodings = tokenize(test_texts)\n",
        "\n",
        "# Create Custom Dataset\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "test_dataset = TextDataset(test_encodings, test_labels)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Model and Optimizer\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=4)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "model.to(device)\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in test_loader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_ghmsiFpzcw",
        "outputId": "0050327f-6723-4a3c-8872-cec9a174f23e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Training Loss: 0.7748873967732957\n",
            "Epoch 2/5, Average Training Loss: 0.5640595891137621\n",
            "Epoch 3/5, Average Training Loss: 0.4514137096814255\n",
            "Epoch 4/5, Average Training Loss: 0.3820714123880685\n",
            "Epoch 5/5, Average Training Loss: 0.32411224989415105\n",
            "Test Accuracy: 0.7932835820895522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's state dictionary\n",
        "torch.save(model.state_dict(), 'hausa_base_model_state_dict.pth')\n",
        "\n"
      ],
      "metadata": {
        "id": "v9UXI5WN00Az"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"lukman.j.aliyu@gmail.com\"\n",
        "!git config --global user.name \"lukmanaj\"\n"
      ],
      "metadata": {
        "id": "UdWpJPAfpc0k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Add base model for hausa\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDbfHSoG1aOR",
        "outputId": "f9ef0164-7734-4caa-d981-25c227e2ddd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 745179a] Add base model for hausa\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 hausa_base_model_state_dict.pth\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "Jd03lXzB4IlS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}